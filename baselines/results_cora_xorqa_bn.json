[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_bn.tsv", "ARES_Prediction": 0.8922448979591836, "ARES_Confidence_Interval": [0.852, 0.933], "Number_of_Examples_in_Evaluation_Set": 490, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.912}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_bn.tsv", "ARES_Prediction": 0.9538095238095239, "ARES_Confidence_Interval": [0.915, 0.992], "Number_of_Examples_in_Evaluation_Set": 490, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.957}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "baselines/cora_xorqa_bn.tsv", "ARES_Prediction": 0.6952380952380952, "ARES_Confidence_Interval": [0.638, 0.752], "Number_of_Examples_in_Evaluation_Set": 490, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.929}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "baselines/cora_xorqa_bn.tsv", "ARES_Prediction": 1.0269387755102042, "ARES_Confidence_Interval": [0.969, 1.085], "Number_of_Examples_in_Evaluation_Set": 490, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.847}]]