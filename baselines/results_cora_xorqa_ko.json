[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_ko.tsv", "ARES_Prediction": 0.8298942917547568, "ARES_Confidence_Interval": [0.785, 0.875], "Number_of_Examples_in_Evaluation_Set": 473, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.85}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_ko.tsv", "ARES_Prediction": 0.9522692036645526, "ARES_Confidence_Interval": [0.914, 0.991], "Number_of_Examples_in_Evaluation_Set": 473, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.956}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "baselines/cora_xorqa_ko.tsv", "ARES_Prediction": 0.6821000704721636, "ARES_Confidence_Interval": [0.624, 0.74], "Number_of_Examples_in_Evaluation_Set": 473, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.915}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "baselines/cora_xorqa_ko.tsv", "ARES_Prediction": 1.0277801268498943, "ARES_Confidence_Interval": [0.97, 1.086], "Number_of_Examples_in_Evaluation_Set": 473, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.848}]]