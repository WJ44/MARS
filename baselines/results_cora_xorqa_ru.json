[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_ru.tsv", "ARES_Prediction": 0.8513163064833006, "ARES_Confidence_Interval": [0.813, 0.889], "Number_of_Examples_in_Evaluation_Set": 1018, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.871}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_ru.tsv", "ARES_Prediction": 0.9730910281597904, "ARES_Confidence_Interval": [0.938, 1.008], "Number_of_Examples_in_Evaluation_Set": 1018, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.976}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "baselines/cora_xorqa_ru.tsv", "ARES_Prediction": 0.677275703994761, "ARES_Confidence_Interval": [0.622, 0.732], "Number_of_Examples_in_Evaluation_Set": 1018, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.911}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "baselines/cora_xorqa_ru.tsv", "ARES_Prediction": 0.8224361493123773, "ARES_Confidence_Interval": [0.766, 0.879], "Number_of_Examples_in_Evaluation_Set": 1018, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.642}]]