[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_te.tsv", "ARES_Prediction": 0.8487943262411347, "ARES_Confidence_Interval": [0.806, 0.891], "Number_of_Examples_in_Evaluation_Set": 564, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.869}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "baselines/cora_xorqa_te.tsv", "ARES_Prediction": 0.9718439716312057, "ARES_Confidence_Interval": [0.936, 1.008], "Number_of_Examples_in_Evaluation_Set": 564, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.975}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "baselines/cora_xorqa_te.tsv", "ARES_Prediction": 0.6744680851063829, "ARES_Confidence_Interval": [0.617, 0.732], "Number_of_Examples_in_Evaluation_Set": 564, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.908}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "baselines/cora_xorqa_te.tsv", "ARES_Prediction": 1.0381560283687943, "ARES_Confidence_Interval": [0.982, 1.094], "Number_of_Examples_in_Evaluation_Set": 564, "Ground_Truth_Performance": null, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": null, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.858}]]