[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.4746666666666668, "ARES_Confidence_Interval": [0.415, 0.534], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.878, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.558}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5118570474281897, "ARES_Confidence_Interval": [0.453, 0.571], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.595}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5379091516366065, "ARES_Confidence_Interval": [0.479, 0.597], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.621}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.5499331997327989, "ARES_Confidence_Interval": [0.491, 0.609], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.882, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.633}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.5786666666666667, "ARES_Confidence_Interval": [0.521, 0.637], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.662}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.5880093520374081, "ARES_Confidence_Interval": [0.53, 0.646], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.671}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.6146666666666666, "ARES_Confidence_Interval": [0.557, 0.672], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.924, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.698}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6240814963259853, "ARES_Confidence_Interval": [0.567, 0.681], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.896, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.707}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.6546666666666666, "ARES_Confidence_Interval": [0.599, 0.711], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.942, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.738}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.52, "ARES_Confidence_Interval": [0.467, 0.573], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.936, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.52}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5531062124248497, "ARES_Confidence_Interval": [0.501, 0.606], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.553}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5691382765531062, "ARES_Confidence_Interval": [0.517, 0.621], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.569}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.6012024048096193, "ARES_Confidence_Interval": [0.549, 0.653], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.601}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.616, "ARES_Confidence_Interval": [0.564, 0.668], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.616}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.627254509018036, "ARES_Confidence_Interval": [0.576, 0.679], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.627}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.668, "ARES_Confidence_Interval": [0.617, 0.719], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.668}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6933867735470942, "ARES_Confidence_Interval": [0.643, 0.743], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.693}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.706, "ARES_Confidence_Interval": [0.657, 0.755], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.706}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.44066666666666665, "ARES_Confidence_Interval": [0.373, 0.508], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.718, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.734}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.498249832999332, "ARES_Confidence_Interval": [0.432, 0.564], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.709, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.792}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.4862257849031396, "ARES_Confidence_Interval": [0.42, 0.553], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.752, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.78}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.4862257849031396, "ARES_Confidence_Interval": [0.42, 0.553], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.747, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.78}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.5126666666666667, "ARES_Confidence_Interval": [0.447, 0.578], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.766, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.806}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.5303139612558451, "ARES_Confidence_Interval": [0.466, 0.595], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.762, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.824}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.5066666666666667, "ARES_Confidence_Interval": [0.441, 0.572], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.802, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.8}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.5323179692718771, "ARES_Confidence_Interval": [0.468, 0.597], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.798, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.826}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.5626666666666666, "ARES_Confidence_Interval": [0.499, 0.626], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.808, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.856}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.4926666666666667, "ARES_Confidence_Interval": [0.44, 0.545], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.526}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5458249832999332, "ARES_Confidence_Interval": [0.494, 0.598], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.579}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5558450233800936, "ARES_Confidence_Interval": [0.504, 0.608], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.589}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.5638610554442218, "ARES_Confidence_Interval": [0.512, 0.616], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.942, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.597}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.5846666666666667, "ARES_Confidence_Interval": [0.533, 0.636], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.618}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.6099532398129592, "ARES_Confidence_Interval": [0.559, 0.661], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.95, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.643}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.6246666666666667, "ARES_Confidence_Interval": [0.574, 0.675], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.956, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.658}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6560454241816968, "ARES_Confidence_Interval": [0.606, 0.706], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.942, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.689}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.6786666666666666, "ARES_Confidence_Interval": [0.63, 0.728], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.956, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.712}]]