[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.4637387387387387, "ARES_Confidence_Interval": [0.413, 0.514], "Number_of_Examples_in_Evaluation_Set": 1184, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.89, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.4790298728186927, "ARES_Confidence_Interval": [0.428, 0.53], "Number_of_Examples_in_Evaluation_Set": 1127, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.886, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.5058240396530359, "ARES_Confidence_Interval": [0.455, 0.557], "Number_of_Examples_in_Evaluation_Set": 1076, "Ground_Truth_Performance": 0.55, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.888, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.5203109815354714, "ARES_Confidence_Interval": [0.469, 0.572], "Number_of_Examples_in_Evaluation_Set": 1029, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.883, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5459093982420554, "ARES_Confidence_Interval": [0.494, 0.597], "Number_of_Examples_in_Evaluation_Set": 986, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.884, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.5721928898275256, "ARES_Confidence_Interval": [0.521, 0.624], "Number_of_Examples_in_Evaluation_Set": 947, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.898, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.5816849816849817, "ARES_Confidence_Interval": [0.53, 0.634], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.895, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.5889775750665146, "ARES_Confidence_Interval": [0.537, 0.641], "Number_of_Examples_in_Evaluation_Set": 877, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.889, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.632741617357002, "ARES_Confidence_Interval": [0.581, 0.685], "Number_of_Examples_in_Evaluation_Set": 845, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.89, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.475427927927928, "ARES_Confidence_Interval": [0.426, 0.525], "Number_of_Examples_in_Evaluation_Set": 1184, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.894, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.4952410529429163, "ARES_Confidence_Interval": [0.446, 0.545], "Number_of_Examples_in_Evaluation_Set": 1127, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.876, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.5130359355638167, "ARES_Confidence_Interval": [0.463, 0.563], "Number_of_Examples_in_Evaluation_Set": 1076, "Ground_Truth_Performance": 0.55, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.886, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.5351700680272109, "ARES_Confidence_Interval": [0.485, 0.585], "Number_of_Examples_in_Evaluation_Set": 1029, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5589519945909398, "ARES_Confidence_Interval": [0.508, 0.609], "Number_of_Examples_in_Evaluation_Set": 986, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.887, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.5589616332277367, "ARES_Confidence_Interval": [0.508, 0.61], "Number_of_Examples_in_Evaluation_Set": 947, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.879, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.5894871794871795, "ARES_Confidence_Interval": [0.539, 0.64], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.899, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.6092398327632079, "ARES_Confidence_Interval": [0.558, 0.66], "Number_of_Examples_in_Evaluation_Set": 877, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.879, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.6380078895463511, "ARES_Confidence_Interval": [0.587, 0.689], "Number_of_Examples_in_Evaluation_Set": 845, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.899, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.5220720720720721, "ARES_Confidence_Interval": [0.457, 0.587], "Number_of_Examples_in_Evaluation_Set": 1184, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.698, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.5383614315291334, "ARES_Confidence_Interval": [0.473, 0.604], "Number_of_Examples_in_Evaluation_Set": 1127, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.708, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.5209417596034697, "ARES_Confidence_Interval": [0.455, 0.587], "Number_of_Examples_in_Evaluation_Set": 1076, "Ground_Truth_Performance": 0.55, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.732, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.5566569484936832, "ARES_Confidence_Interval": [0.491, 0.622], "Number_of_Examples_in_Evaluation_Set": 1029, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.71, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5745774171737661, "ARES_Confidence_Interval": [0.509, 0.64], "Number_of_Examples_in_Evaluation_Set": 986, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.732, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.5593804998240056, "ARES_Confidence_Interval": [0.493, 0.626], "Number_of_Examples_in_Evaluation_Set": 947, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.751, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.5754578754578755, "ARES_Confidence_Interval": [0.509, 0.642], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.735, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.5850247054351958, "ARES_Confidence_Interval": [0.519, 0.651], "Number_of_Examples_in_Evaluation_Set": 877, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.738, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.6169625246548324, "ARES_Confidence_Interval": [0.551, 0.683], "Number_of_Examples_in_Evaluation_Set": 845, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.773, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.49856981981981985, "ARES_Confidence_Interval": [0.459, 0.538], "Number_of_Examples_in_Evaluation_Set": 1184, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.923, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.5162703342206447, "ARES_Confidence_Interval": [0.476, 0.557], "Number_of_Examples_in_Evaluation_Set": 1127, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.548909541511772, "ARES_Confidence_Interval": [0.508, 0.59], "Number_of_Examples_in_Evaluation_Set": 1076, "Ground_Truth_Performance": 0.55, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.578804664723032, "ARES_Confidence_Interval": [0.537, 0.62], "Number_of_Examples_in_Evaluation_Set": 1029, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5940432724814063, "ARES_Confidence_Interval": [0.552, 0.636], "Number_of_Examples_in_Evaluation_Set": 986, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.924, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.5998275255191834, "ARES_Confidence_Interval": [0.558, 0.642], "Number_of_Examples_in_Evaluation_Set": 947, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.6213553113553113, "ARES_Confidence_Interval": [0.579, 0.664], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.6556480425693653, "ARES_Confidence_Interval": [0.613, 0.698], "Number_of_Examples_in_Evaluation_Set": 877, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.6741025641025641, "ARES_Confidence_Interval": [0.631, 0.717], "Number_of_Examples_in_Evaluation_Set": 845, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.909, "Annotated_Examples_used_for_PPI": 300}]]