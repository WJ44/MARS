[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.44933333333333336, "ARES_Confidence_Interval": [0.389, 0.51], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.908, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.516}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.4704074816299265, "ARES_Confidence_Interval": [0.41, 0.531], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.888, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.537}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.5124916499665999, "ARES_Confidence_Interval": [0.452, 0.573], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.579}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.5265197060788244, "ARES_Confidence_Interval": [0.467, 0.586], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.874, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.593}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5313333333333333, "ARES_Confidence_Interval": [0.471, 0.591], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.894, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.598}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.5645958583834335, "ARES_Confidence_Interval": [0.505, 0.624], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.878, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.631}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.5873333333333334, "ARES_Confidence_Interval": [0.528, 0.646], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.908, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.654}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.5946559786239145, "ARES_Confidence_Interval": [0.536, 0.653], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.898, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.661}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.6313333333333333, "ARES_Confidence_Interval": [0.573, 0.689], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.886, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.698}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.4793333333333334, "ARES_Confidence_Interval": [0.42, 0.539], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.88, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.536}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.49042752171008686, "ARES_Confidence_Interval": [0.431, 0.55], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.874, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.547}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.5104676018704075, "ARES_Confidence_Interval": [0.451, 0.57], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.884, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.567}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.526499665998664, "ARES_Confidence_Interval": [0.467, 0.586], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.88, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.583}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5553333333333333, "ARES_Confidence_Interval": [0.497, 0.614], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.888, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.612}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.5605678022712091, "ARES_Confidence_Interval": [0.502, 0.619], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.892, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.617}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.6173333333333334, "ARES_Confidence_Interval": [0.56, 0.675], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.674}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.620688042752171, "ARES_Confidence_Interval": [0.563, 0.678], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.898, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.677}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.6033333333333334, "ARES_Confidence_Interval": [0.545, 0.661], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.884, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.66}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.5346666666666667, "ARES_Confidence_Interval": [0.462, 0.607], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.684, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.668}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.5259853039412158, "ARES_Confidence_Interval": [0.453, 0.598], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.725, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.659}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.564061456245825, "ARES_Confidence_Interval": [0.492, 0.636], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.729, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.697}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.5360053440213761, "ARES_Confidence_Interval": [0.464, 0.608], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.697, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.669}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5726666666666667, "ARES_Confidence_Interval": [0.501, 0.644], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.742, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.706}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.5901135604542418, "ARES_Confidence_Interval": [0.519, 0.661], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.725, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.723}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.5806666666666667, "ARES_Confidence_Interval": [0.509, 0.652], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.764, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.714}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.5820975283901136, "ARES_Confidence_Interval": [0.511, 0.653], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.723, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.715}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.6106666666666667, "ARES_Confidence_Interval": [0.54, 0.681], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.78, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.744}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_ar.tsv", "ARES_Prediction": 0.5033333333333334, "ARES_Confidence_Interval": [0.451, 0.555], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.46}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_ar.tsv", "ARES_Prediction": 0.5102672010688043, "ARES_Confidence_Interval": [0.458, 0.562], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.467}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_ar.tsv", "ARES_Prediction": 0.5403273213092852, "ARES_Confidence_Interval": [0.488, 0.592], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.918, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.497}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_ar.tsv", "ARES_Prediction": 0.56437541750167, "ARES_Confidence_Interval": [0.512, 0.616], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.521}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_ar.tsv", "ARES_Prediction": 0.5673333333333334, "ARES_Confidence_Interval": [0.515, 0.619], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.524}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_ar.tsv", "ARES_Prediction": 0.6164796259185037, "ARES_Confidence_Interval": [0.565, 0.668], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.936, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.573}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_ar.tsv", "ARES_Prediction": 0.6153333333333333, "ARES_Confidence_Interval": [0.564, 0.667], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.894, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.572}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_ar.tsv", "ARES_Prediction": 0.6545557782231128, "ARES_Confidence_Interval": [0.603, 0.706], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.611}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_ar.tsv", "ARES_Prediction": 0.6773333333333333, "ARES_Confidence_Interval": [0.627, 0.728], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.634}]]