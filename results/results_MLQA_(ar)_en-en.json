[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.5155311355311356, "ARES_Confidence_Interval": [0.457, 0.574], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.92, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5384778420038536, "ARES_Confidence_Interval": [0.479, 0.598], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.951, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5512121212121213, "ARES_Confidence_Interval": [0.491, 0.611], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.952, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5645991561181435, "ARES_Confidence_Interval": [0.504, 0.625], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.943, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5870957095709571, "ARES_Confidence_Interval": [0.526, 0.648], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.957, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.623127147766323, "ARES_Confidence_Interval": [0.562, 0.684], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.6240476190476191, "ARES_Confidence_Interval": [0.562, 0.686], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.961, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6696530359355639, "ARES_Confidence_Interval": [0.608, 0.731], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.97, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6864102564102564, "ARES_Confidence_Interval": [0.625, 0.748], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.969, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.4382417582417582, "ARES_Confidence_Interval": [0.376, 0.5], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.47624277456647396, "ARES_Confidence_Interval": [0.414, 0.539], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5481818181818181, "ARES_Confidence_Interval": [0.486, 0.611], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5502531645569619, "ARES_Confidence_Interval": [0.487, 0.614], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.937, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5603630363036303, "ARES_Confidence_Interval": [0.496, 0.625], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5760481099656358, "ARES_Confidence_Interval": [0.511, 0.641], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5907142857142857, "ARES_Confidence_Interval": [0.526, 0.656], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.939, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6251672862453532, "ARES_Confidence_Interval": [0.56, 0.69], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6453846153846154, "ARES_Confidence_Interval": [0.581, 0.71], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.4982417582417582, "ARES_Confidence_Interval": [0.432, 0.565], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.731, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5058959537572254, "ARES_Confidence_Interval": [0.439, 0.573], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.749, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5096969696969696, "ARES_Confidence_Interval": [0.442, 0.577], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.77, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.559620253164557, "ARES_Confidence_Interval": [0.494, 0.625], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.731, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5452805280528052, "ARES_Confidence_Interval": [0.479, 0.612], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.789, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5716151202749141, "ARES_Confidence_Interval": [0.506, 0.637], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.787, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5507142857142857, "ARES_Confidence_Interval": [0.483, 0.618], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.818, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.5950185873605948, "ARES_Confidence_Interval": [0.53, 0.66], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.822, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.5861538461538461, "ARES_Confidence_Interval": [0.52, 0.652], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.846, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.45274725274725275, "ARES_Confidence_Interval": [0.385, 0.521], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.753, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.500578034682081, "ARES_Confidence_Interval": [0.43, 0.571], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.763, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5363636363636364, "ARES_Confidence_Interval": [0.464, 0.609], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.773, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5512658227848102, "ARES_Confidence_Interval": [0.478, 0.625], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.75, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.563036303630363, "ARES_Confidence_Interval": [0.488, 0.638], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.762, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5676975945017182, "ARES_Confidence_Interval": [0.492, 0.643], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.729, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5607142857142857, "ARES_Confidence_Interval": [0.484, 0.637], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.696, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6052044609665428, "ARES_Confidence_Interval": [0.527, 0.683], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.729, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6346153846153846, "ARES_Confidence_Interval": [0.555, 0.714], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.719, "Annotated_Examples_used_for_PPI": 300}]]