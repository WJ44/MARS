[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_all.tsv", "ARES_Prediction": 0.5279041916167665, "ARES_Confidence_Interval": [0.484, 0.572], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_all.tsv", "ARES_Prediction": 0.5355555555555556, "ARES_Confidence_Interval": [0.491, 0.58], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.913, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_all.tsv", "ARES_Prediction": 0.5602197802197803, "ARES_Confidence_Interval": [0.515, 0.605], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.92, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_all.tsv", "ARES_Prediction": 0.5701262916188289, "ARES_Confidence_Interval": [0.524, 0.616], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_all.tsv", "ARES_Prediction": 0.595568862275449, "ARES_Confidence_Interval": [0.55, 0.641], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_all.tsv", "ARES_Prediction": 0.6017228464419475, "ARES_Confidence_Interval": [0.555, 0.648], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_all.tsv", "ARES_Prediction": 0.6267532467532467, "ARES_Confidence_Interval": [0.58, 0.673], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_all.tsv", "ARES_Prediction": 0.663288409703504, "ARES_Confidence_Interval": [0.617, 0.71], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_all.tsv", "ARES_Prediction": 0.6695104895104895, "ARES_Confidence_Interval": [0.623, 0.716], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_all.tsv", "ARES_Prediction": 0.5226147704590819, "ARES_Confidence_Interval": [0.477, 0.569], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_all.tsv", "ARES_Prediction": 0.5427882599580713, "ARES_Confidence_Interval": [0.496, 0.589], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.925, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_all.tsv", "ARES_Prediction": 0.566996336996337, "ARES_Confidence_Interval": [0.52, 0.614], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_all.tsv", "ARES_Prediction": 0.5902372751626483, "ARES_Confidence_Interval": [0.543, 0.637], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_all.tsv", "ARES_Prediction": 0.6254091816367265, "ARES_Confidence_Interval": [0.578, 0.673], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_all.tsv", "ARES_Prediction": 0.6358676654182273, "ARES_Confidence_Interval": [0.588, 0.683], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_all.tsv", "ARES_Prediction": 0.6641991341991342, "ARES_Confidence_Interval": [0.617, 0.712], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_all.tsv", "ARES_Prediction": 0.655696316262354, "ARES_Confidence_Interval": [0.608, 0.704], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.946, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_all.tsv", "ARES_Prediction": 0.6763869463869464, "ARES_Confidence_Interval": [0.628, 0.725], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.923, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_all.tsv", "ARES_Prediction": 0.5171656686626747, "ARES_Confidence_Interval": [0.459, 0.576], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.714, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_all.tsv", "ARES_Prediction": 0.5287211740041928, "ARES_Confidence_Interval": [0.47, 0.587], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.721, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_all.tsv", "ARES_Prediction": 0.5523809523809524, "ARES_Confidence_Interval": [0.494, 0.611], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.741, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_all.tsv", "ARES_Prediction": 0.5588595484117873, "ARES_Confidence_Interval": [0.5, 0.618], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.755, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_all.tsv", "ARES_Prediction": 0.5439121756487026, "ARES_Confidence_Interval": [0.485, 0.603], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.804, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_all.tsv", "ARES_Prediction": 0.5469413233458178, "ARES_Confidence_Interval": [0.487, 0.606], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.793, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_all.tsv", "ARES_Prediction": 0.577056277056277, "ARES_Confidence_Interval": [0.518, 0.636], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.814, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_all.tsv", "ARES_Prediction": 0.6049415992812219, "ARES_Confidence_Interval": [0.546, 0.663], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.799, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_all.tsv", "ARES_Prediction": 0.6016317016317017, "ARES_Confidence_Interval": [0.543, 0.66], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.824, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_all.tsv", "ARES_Prediction": 0.5652295409181637, "ARES_Confidence_Interval": [0.508, 0.622], "Number_of_Examples_in_Evaluation_Set": 1002, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.787, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_all.tsv", "ARES_Prediction": 0.5783228511530398, "ARES_Confidence_Interval": [0.521, 0.636], "Number_of_Examples_in_Evaluation_Set": 954, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.787, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_all.tsv", "ARES_Prediction": 0.6074725274725274, "ARES_Confidence_Interval": [0.55, 0.665], "Number_of_Examples_in_Evaluation_Set": 910, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.815, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_all.tsv", "ARES_Prediction": 0.5990585533869116, "ARES_Confidence_Interval": [0.541, 0.657], "Number_of_Examples_in_Evaluation_Set": 871, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.777, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_all.tsv", "ARES_Prediction": 0.5811976047904192, "ARES_Confidence_Interval": [0.523, 0.64], "Number_of_Examples_in_Evaluation_Set": 835, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.765, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_all.tsv", "ARES_Prediction": 0.6281897627965043, "ARES_Confidence_Interval": [0.569, 0.687], "Number_of_Examples_in_Evaluation_Set": 801, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.758, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_all.tsv", "ARES_Prediction": 0.6436363636363636, "ARES_Confidence_Interval": [0.584, 0.703], "Number_of_Examples_in_Evaluation_Set": 770, "Ground_Truth_Performance": 0.651, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.756, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_all.tsv", "ARES_Prediction": 0.6853908355795149, "ARES_Confidence_Interval": [0.625, 0.745], "Number_of_Examples_in_Evaluation_Set": 742, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.776, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_all.tsv", "ARES_Prediction": 0.675104895104895, "ARES_Confidence_Interval": [0.615, 0.736], "Number_of_Examples_in_Evaluation_Set": 715, "Ground_Truth_Performance": 0.701, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.727, "Annotated_Examples_used_for_PPI": 300}]]