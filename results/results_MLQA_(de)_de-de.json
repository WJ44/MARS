[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.49157509157509155, "ARES_Confidence_Interval": [0.428, 0.555], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.898, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.5324662813102119, "ARES_Confidence_Interval": [0.468, 0.597], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.913, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5196969696969697, "ARES_Confidence_Interval": [0.454, 0.585], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5339662447257384, "ARES_Confidence_Interval": [0.468, 0.6], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.905, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.5773927392739273, "ARES_Confidence_Interval": [0.511, 0.644], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.927, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.5881443298969071, "ARES_Confidence_Interval": [0.521, 0.656], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.6047619047619047, "ARES_Confidence_Interval": [0.537, 0.673], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.907, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6376084262701363, "ARES_Confidence_Interval": [0.57, 0.706], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.963, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.6371794871794871, "ARES_Confidence_Interval": [0.568, 0.706], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.869, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.5268864468864469, "ARES_Confidence_Interval": [0.468, 0.586], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.909, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.540019267822736, "ARES_Confidence_Interval": [0.48, 0.6], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.908, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5966666666666667, "ARES_Confidence_Interval": [0.537, 0.657], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.909, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5916033755274261, "ARES_Confidence_Interval": [0.53, 0.653], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.6138283828382839, "ARES_Confidence_Interval": [0.552, 0.676], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.6495876288659794, "ARES_Confidence_Interval": [0.588, 0.711], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.897, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.6466666666666667, "ARES_Confidence_Interval": [0.584, 0.709], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.943, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6658116480793062, "ARES_Confidence_Interval": [0.603, 0.729], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.948, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.688974358974359, "ARES_Confidence_Interval": [0.626, 0.752], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.47901098901098904, "ARES_Confidence_Interval": [0.411, 0.547], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.734, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.5145664739884392, "ARES_Confidence_Interval": [0.447, 0.582], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.734, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.4945454545454545, "ARES_Confidence_Interval": [0.425, 0.564], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.767, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5089873417721519, "ARES_Confidence_Interval": [0.44, 0.578], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.782, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.5320792079207921, "ARES_Confidence_Interval": [0.463, 0.601], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.789, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.5269415807560137, "ARES_Confidence_Interval": [0.457, 0.597], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.797, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.5221428571428571, "ARES_Confidence_Interval": [0.452, 0.593], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.854, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.5541263940520446, "ARES_Confidence_Interval": [0.485, 0.623], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.862, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.5823076923076923, "ARES_Confidence_Interval": [0.515, 0.65], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.827, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.4989010989010989, "ARES_Confidence_Interval": [0.432, 0.566], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.49393063583815033, "ARES_Confidence_Interval": [0.426, 0.561], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.789, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.55, "ARES_Confidence_Interval": [0.48, 0.62], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.818, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5360759493670886, "ARES_Confidence_Interval": [0.466, 0.606], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.785, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.555940594059406, "ARES_Confidence_Interval": [0.484, 0.627], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.759, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.5898625429553265, "ARES_Confidence_Interval": [0.517, 0.663], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.575, "ARES_Confidence_Interval": [0.501, 0.649], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.761, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6184014869888476, "ARES_Confidence_Interval": [0.543, 0.693], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.777, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.6076923076923076, "ARES_Confidence_Interval": [0.532, 0.683], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.735, "Annotated_Examples_used_for_PPI": 300}]]