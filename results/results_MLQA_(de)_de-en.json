[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.5047252747252747, "ARES_Confidence_Interval": [0.444, 0.566], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.893, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5233526011560693, "ARES_Confidence_Interval": [0.461, 0.585], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.925, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5315151515151515, "ARES_Confidence_Interval": [0.468, 0.595], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.897, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.5432911392405063, "ARES_Confidence_Interval": [0.479, 0.607], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.918, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5806600660066007, "ARES_Confidence_Interval": [0.516, 0.645], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.927, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.6157388316151202, "ARES_Confidence_Interval": [0.551, 0.68], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6121428571428571, "ARES_Confidence_Interval": [0.547, 0.678], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.918, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6602973977695167, "ARES_Confidence_Interval": [0.595, 0.725], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6569230769230769, "ARES_Confidence_Interval": [0.591, 0.723], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.923, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.45157509157509157, "ARES_Confidence_Interval": [0.39, 0.513], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.4895761078998073, "ARES_Confidence_Interval": [0.427, 0.552], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5615151515151515, "ARES_Confidence_Interval": [0.499, 0.624], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.5635864978902954, "ARES_Confidence_Interval": [0.5, 0.627], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.937, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5736963696369637, "ARES_Confidence_Interval": [0.509, 0.638], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.5893814432989691, "ARES_Confidence_Interval": [0.524, 0.654], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.604047619047619, "ARES_Confidence_Interval": [0.539, 0.669], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.939, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6385006195786865, "ARES_Confidence_Interval": [0.574, 0.703], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6587179487179488, "ARES_Confidence_Interval": [0.594, 0.724], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.5070329670329671, "ARES_Confidence_Interval": [0.439, 0.576], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.739, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5558959537572254, "ARES_Confidence_Interval": [0.489, 0.623], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.731, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5324242424242425, "ARES_Confidence_Interval": [0.463, 0.602], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.767, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.5083544303797469, "ARES_Confidence_Interval": [0.438, 0.579], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5523762376237624, "ARES_Confidence_Interval": [0.483, 0.622], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.805, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.5494501718213058, "ARES_Confidence_Interval": [0.479, 0.62], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.797, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.5864285714285714, "ARES_Confidence_Interval": [0.517, 0.656], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.775, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.5929739776951674, "ARES_Confidence_Interval": [0.524, 0.662], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.836, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6015384615384616, "ARES_Confidence_Interval": [0.532, 0.671], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.842, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.49326007326007326, "ARES_Confidence_Interval": [0.427, 0.559], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.808, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5866666666666667, "ARES_Confidence_Interval": [0.519, 0.654], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.812, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5775757575757576, "ARES_Confidence_Interval": [0.509, 0.646], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.855, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.6151476793248946, "ARES_Confidence_Interval": [0.546, 0.685], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.864, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5718151815181518, "ARES_Confidence_Interval": [0.501, 0.642], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.792, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.6021305841924398, "ARES_Confidence_Interval": [0.531, 0.673], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.828, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6080952380952381, "ARES_Confidence_Interval": [0.536, 0.68], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.8, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6219826517967781, "ARES_Confidence_Interval": [0.549, 0.695], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.784, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6751282051282051, "ARES_Confidence_Interval": [0.602, 0.748], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.835, "Annotated_Examples_used_for_PPI": 300}]]