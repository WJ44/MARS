[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.49733333333333335, "ARES_Confidence_Interval": [0.44, 0.555], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.534}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5304676018704075, "ARES_Confidence_Interval": [0.473, 0.588], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.89, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.567}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5424916499665999, "ARES_Confidence_Interval": [0.485, 0.6], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.579}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.5745557782231129, "ARES_Confidence_Interval": [0.518, 0.631], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.884, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.611}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5553333333333333, "ARES_Confidence_Interval": [0.498, 0.612], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.912, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.592}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.5525116900467603, "ARES_Confidence_Interval": [0.495, 0.61], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.589}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6013333333333334, "ARES_Confidence_Interval": [0.545, 0.658], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.638}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6366800267201069, "ARES_Confidence_Interval": [0.581, 0.692], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.673}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6413333333333334, "ARES_Confidence_Interval": [0.586, 0.697], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.678}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.5126666666666667, "ARES_Confidence_Interval": [0.461, 0.565], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.536}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5317768871075484, "ARES_Confidence_Interval": [0.48, 0.584], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.555}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5678490313961256, "ARES_Confidence_Interval": [0.516, 0.619], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.932, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.591}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.5618370073480294, "ARES_Confidence_Interval": [0.51, 0.614], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.585}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5906666666666667, "ARES_Confidence_Interval": [0.539, 0.642], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.614}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.611937207748831, "ARES_Confidence_Interval": [0.561, 0.663], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.902, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.635}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6386666666666667, "ARES_Confidence_Interval": [0.588, 0.689], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.924, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.662}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6640414161656647, "ARES_Confidence_Interval": [0.614, 0.714], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.952, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.687}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6706666666666666, "ARES_Confidence_Interval": [0.621, 0.72], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.694}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.5666666666666667, "ARES_Confidence_Interval": [0.501, 0.632], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.716, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.76}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5601736806947227, "ARES_Confidence_Interval": [0.495, 0.626], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.727, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.754}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5721977287909151, "ARES_Confidence_Interval": [0.507, 0.637], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.754, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.766}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.568189712758851, "ARES_Confidence_Interval": [0.503, 0.633], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.758, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.762}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5706666666666667, "ARES_Confidence_Interval": [0.506, 0.636], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.768, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.764}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.5641816967267869, "ARES_Confidence_Interval": [0.499, 0.629], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.8, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.758}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.5926666666666667, "ARES_Confidence_Interval": [0.528, 0.657], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.8, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.786}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.5942418169672679, "ARES_Confidence_Interval": [0.53, 0.659], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.824, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.788}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.5866666666666667, "ARES_Confidence_Interval": [0.522, 0.651], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.84, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.78}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.5653333333333334, "ARES_Confidence_Interval": [0.502, 0.629], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.838, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.462}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5382030728122913, "ARES_Confidence_Interval": [0.475, 0.602], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.806, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.435}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5802872411489646, "ARES_Confidence_Interval": [0.517, 0.644], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.818, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.477}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.6103473613894455, "ARES_Confidence_Interval": [0.547, 0.674], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.816, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.507}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.6313333333333333, "ARES_Confidence_Interval": [0.568, 0.695], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.872, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.528}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.6424114896459585, "ARES_Confidence_Interval": [0.579, 0.706], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.81, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.539}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6233333333333333, "ARES_Confidence_Interval": [0.56, 0.687], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.52}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6444154976619907, "ARES_Confidence_Interval": [0.581, 0.708], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.798, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.541}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6813333333333333, "ARES_Confidence_Interval": [0.618, 0.745], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.786, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.578}]]