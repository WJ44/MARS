[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.5133333333333334, "ARES_Confidence_Interval": [0.462, 0.565], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.92, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.56}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5445156980627923, "ARES_Confidence_Interval": [0.494, 0.596], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.591}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5605477621910488, "ARES_Confidence_Interval": [0.51, 0.611], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.936, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.607}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5765798263193053, "ARES_Confidence_Interval": [0.526, 0.627], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.928, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.623}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5973333333333334, "ARES_Confidence_Interval": [0.547, 0.647], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.928, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.644}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.6066399465597863, "ARES_Confidence_Interval": [0.557, 0.656], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.952, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.653}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.6313333333333334, "ARES_Confidence_Interval": [0.582, 0.681], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.948, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.678}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6707682030728124, "ARES_Confidence_Interval": [0.623, 0.719], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.946, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.717}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6693333333333333, "ARES_Confidence_Interval": [0.621, 0.717], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.948, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.716}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.4960000000000001, "ARES_Confidence_Interval": [0.439, 0.553], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.536}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5151102204408817, "ARES_Confidence_Interval": [0.458, 0.572], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.555}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5511823647294589, "ARES_Confidence_Interval": [0.495, 0.608], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.932, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.591}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5451703406813627, "ARES_Confidence_Interval": [0.488, 0.602], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.585}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.574, "ARES_Confidence_Interval": [0.518, 0.63], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.614}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5952705410821643, "ARES_Confidence_Interval": [0.539, 0.651], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.902, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.635}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.622, "ARES_Confidence_Interval": [0.567, 0.677], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.924, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.662}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.647374749498998, "ARES_Confidence_Interval": [0.593, 0.702], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.952, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.687}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6539999999999999, "ARES_Confidence_Interval": [0.599, 0.709], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.694}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.48, "ARES_Confidence_Interval": [0.417, 0.543], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.756, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.74}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5436072144288577, "ARES_Confidence_Interval": [0.483, 0.604], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.717, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.804}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5355911823647295, "ARES_Confidence_Interval": [0.475, 0.597], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.747, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.796}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5536272545090181, "ARES_Confidence_Interval": [0.493, 0.614], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.749, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.814}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.54, "ARES_Confidence_Interval": [0.479, 0.601], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.792, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.8}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5716633266533067, "ARES_Confidence_Interval": [0.512, 0.631], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.774, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.832}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.588, "ARES_Confidence_Interval": [0.529, 0.647], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.798, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.848}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.5917034068136272, "ARES_Confidence_Interval": [0.533, 0.65], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.82, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.852}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.596, "ARES_Confidence_Interval": [0.538, 0.654], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.828, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.856}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.5186666666666666, "ARES_Confidence_Interval": [0.451, 0.586], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.756, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.312}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5393319973279893, "ARES_Confidence_Interval": [0.472, 0.607], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.743, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.333}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5393319973279893, "ARES_Confidence_Interval": [0.472, 0.607], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.705, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.333}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5794121576486306, "ARES_Confidence_Interval": [0.511, 0.648], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.745, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.373}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5986666666666667, "ARES_Confidence_Interval": [0.53, 0.667], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.744, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.392}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5774081496325985, "ARES_Confidence_Interval": [0.509, 0.646], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.677, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.371}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.6066666666666667, "ARES_Confidence_Interval": [0.538, 0.675], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.714, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.4}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.591436205744823, "ARES_Confidence_Interval": [0.523, 0.66], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.677, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.385}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6586666666666667, "ARES_Confidence_Interval": [0.59, 0.728], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.7, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.452}]]