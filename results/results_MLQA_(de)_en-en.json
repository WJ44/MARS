[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.5155311355311356, "ARES_Confidence_Interval": [0.457, 0.574], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.92, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5384778420038536, "ARES_Confidence_Interval": [0.479, 0.598], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.951, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5512121212121213, "ARES_Confidence_Interval": [0.491, 0.611], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.952, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5645991561181435, "ARES_Confidence_Interval": [0.504, 0.625], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.943, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5870957095709571, "ARES_Confidence_Interval": [0.526, 0.648], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.957, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.623127147766323, "ARES_Confidence_Interval": [0.562, 0.684], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.6240476190476191, "ARES_Confidence_Interval": [0.562, 0.686], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.961, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6696530359355639, "ARES_Confidence_Interval": [0.608, 0.731], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.97, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6864102564102564, "ARES_Confidence_Interval": [0.625, 0.748], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.969, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.4382417582417582, "ARES_Confidence_Interval": [0.376, 0.5], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.47624277456647396, "ARES_Confidence_Interval": [0.414, 0.539], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5481818181818181, "ARES_Confidence_Interval": [0.486, 0.611], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5502531645569619, "ARES_Confidence_Interval": [0.487, 0.614], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.937, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5603630363036303, "ARES_Confidence_Interval": [0.496, 0.625], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5760481099656358, "ARES_Confidence_Interval": [0.511, 0.641], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5907142857142857, "ARES_Confidence_Interval": [0.526, 0.656], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.939, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6251672862453532, "ARES_Confidence_Interval": [0.56, 0.69], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6453846153846154, "ARES_Confidence_Interval": [0.581, 0.71], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.4982417582417582, "ARES_Confidence_Interval": [0.432, 0.565], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.731, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5058959537572254, "ARES_Confidence_Interval": [0.439, 0.573], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.749, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5096969696969696, "ARES_Confidence_Interval": [0.442, 0.577], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.77, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.559620253164557, "ARES_Confidence_Interval": [0.494, 0.625], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.731, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5452805280528052, "ARES_Confidence_Interval": [0.479, 0.612], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.789, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5716151202749141, "ARES_Confidence_Interval": [0.506, 0.637], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.787, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5507142857142857, "ARES_Confidence_Interval": [0.483, 0.618], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.818, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.5950185873605948, "ARES_Confidence_Interval": [0.53, 0.66], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.822, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.5861538461538461, "ARES_Confidence_Interval": [0.52, 0.652], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.846, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.48296703296703297, "ARES_Confidence_Interval": [0.414, 0.552], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.723, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5265895953757225, "ARES_Confidence_Interval": [0.455, 0.598], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.737, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5575757575757576, "ARES_Confidence_Interval": [0.485, 0.631], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.752, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.5607594936708861, "ARES_Confidence_Interval": [0.487, 0.635], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.741, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.5927392739273927, "ARES_Confidence_Interval": [0.517, 0.668], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.733, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.5848797250859107, "ARES_Confidence_Interval": [0.509, 0.661], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.711, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.5892857142857143, "ARES_Confidence_Interval": [0.512, 0.666], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.668, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6200743494423793, "ARES_Confidence_Interval": [0.542, 0.698], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.714, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6384615384615384, "ARES_Confidence_Interval": [0.559, 0.718], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.715, "Annotated_Examples_used_for_PPI": 300}]]