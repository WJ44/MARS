[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.548, "ARES_Confidence_Interval": [0.495, 0.601], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.878, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.558}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5851903807615231, "ARES_Confidence_Interval": [0.533, 0.638], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.595}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.6112424849699398, "ARES_Confidence_Interval": [0.559, 0.663], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.621}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.6232665330661322, "ARES_Confidence_Interval": [0.571, 0.675], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.882, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.633}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.652, "ARES_Confidence_Interval": [0.601, 0.703], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.662}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.6613426853707415, "ARES_Confidence_Interval": [0.61, 0.712], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.671}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.688, "ARES_Confidence_Interval": [0.638, 0.738], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.924, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.698}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6974148296593187, "ARES_Confidence_Interval": [0.648, 0.747], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.896, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.707}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.728, "ARES_Confidence_Interval": [0.679, 0.777], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.942, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.738}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.5266666666666667, "ARES_Confidence_Interval": [0.474, 0.579], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.936, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.52}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5597728790915164, "ARES_Confidence_Interval": [0.507, 0.612], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.553}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5758049432197729, "ARES_Confidence_Interval": [0.523, 0.628], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.569}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.607869071476286, "ARES_Confidence_Interval": [0.556, 0.66], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.601}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.6226666666666667, "ARES_Confidence_Interval": [0.571, 0.674], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.616}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.6339211756847027, "ARES_Confidence_Interval": [0.582, 0.685], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.627}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.6746666666666667, "ARES_Confidence_Interval": [0.624, 0.725], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.668}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.7000534402137609, "ARES_Confidence_Interval": [0.65, 0.75], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.693}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.7126666666666667, "ARES_Confidence_Interval": [0.663, 0.762], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.706}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.5673333333333334, "ARES_Confidence_Interval": [0.506, 0.628], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.718, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.734}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.6249164996659987, "ARES_Confidence_Interval": [0.566, 0.684], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.709, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.792}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.6128924515698063, "ARES_Confidence_Interval": [0.553, 0.672], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.752, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.78}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.6128924515698063, "ARES_Confidence_Interval": [0.553, 0.672], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.747, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.78}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.6393333333333334, "ARES_Confidence_Interval": [0.581, 0.698], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.766, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.806}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.6569806279225118, "ARES_Confidence_Interval": [0.599, 0.715], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.762, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.824}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.6333333333333334, "ARES_Confidence_Interval": [0.575, 0.692], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.802, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.8}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6589846359385438, "ARES_Confidence_Interval": [0.601, 0.717], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.798, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.826}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.6893333333333334, "ARES_Confidence_Interval": [0.633, 0.745], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.808, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.856}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.5426666666666667, "ARES_Confidence_Interval": [0.49, 0.596], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.526}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5958249832999333, "ARES_Confidence_Interval": [0.543, 0.648], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.93, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.579}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.6058450233800936, "ARES_Confidence_Interval": [0.553, 0.658], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.589}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.6138610554442219, "ARES_Confidence_Interval": [0.561, 0.666], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.942, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.597}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.6346666666666667, "ARES_Confidence_Interval": [0.583, 0.687], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.618}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.6599532398129593, "ARES_Confidence_Interval": [0.608, 0.712], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.95, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.643}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.6746666666666667, "ARES_Confidence_Interval": [0.623, 0.726], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.956, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.658}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.7060454241816968, "ARES_Confidence_Interval": [0.656, 0.756], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.942, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.689}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.7286666666666667, "ARES_Confidence_Interval": [0.679, 0.778], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.956, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.712}]]