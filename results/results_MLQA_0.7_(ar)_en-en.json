[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.5446666666666667, "ARES_Confidence_Interval": [0.492, 0.597], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.558}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5718370073480294, "ARES_Confidence_Interval": [0.52, 0.624], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.936, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.585}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5858650634602539, "ARES_Confidence_Interval": [0.534, 0.638], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.932, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.599}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.6079091516366065, "ARES_Confidence_Interval": [0.556, 0.66], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.621}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.6346666666666667, "ARES_Confidence_Interval": [0.584, 0.686], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.648}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.6580093520374082, "ARES_Confidence_Interval": [0.608, 0.709], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.942, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.671}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.6666666666666667, "ARES_Confidence_Interval": [0.616, 0.717], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.954, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.68}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.6920774883099533, "ARES_Confidence_Interval": [0.643, 0.742], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.958, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.705}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.6986666666666667, "ARES_Confidence_Interval": [0.649, 0.748], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.96, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.712}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.54, "ARES_Confidence_Interval": [0.488, 0.592], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.936, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.52}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.5731062124248497, "ARES_Confidence_Interval": [0.521, 0.625], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.553}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.5891382765531062, "ARES_Confidence_Interval": [0.538, 0.641], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.569}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.6212024048096193, "ARES_Confidence_Interval": [0.57, 0.672], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.601}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.636, "ARES_Confidence_Interval": [0.585, 0.687], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.616}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.647254509018036, "ARES_Confidence_Interval": [0.597, 0.698], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.627}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.6880000000000001, "ARES_Confidence_Interval": [0.638, 0.738], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.668}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.7133867735470942, "ARES_Confidence_Interval": [0.664, 0.762], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.693}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.726, "ARES_Confidence_Interval": [0.677, 0.775], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.706}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.6486666666666667, "ARES_Confidence_Interval": [0.595, 0.702], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.734, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.762}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.6662257849031397, "ARES_Confidence_Interval": [0.613, 0.719], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.745, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.78}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.6802538410153641, "ARES_Confidence_Interval": [0.628, 0.732], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.754, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.794}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.6842618570474283, "ARES_Confidence_Interval": [0.632, 0.736], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.77, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.798}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.7026666666666667, "ARES_Confidence_Interval": [0.652, 0.754], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.776, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.816}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.7223380093520374, "ARES_Confidence_Interval": [0.672, 0.772], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.778, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.836}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.7226666666666667, "ARES_Confidence_Interval": [0.673, 0.773], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.836}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.7363660654642619, "ARES_Confidence_Interval": [0.687, 0.786], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.822, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.85}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.7286666666666667, "ARES_Confidence_Interval": [0.679, 0.778], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.838, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.842}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.5_en_en.tsv", "ARES_Prediction": 0.6206666666666667, "ARES_Confidence_Interval": [0.555, 0.687], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.802, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.314}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.525_en_en.tsv", "ARES_Prediction": 0.6333199732798931, "ARES_Confidence_Interval": [0.567, 0.7], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.786, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.327}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.55_en_en.tsv", "ARES_Prediction": 0.6774081496325985, "ARES_Confidence_Interval": [0.61, 0.745], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.8, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.371}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.575_en_en.tsv", "ARES_Prediction": 0.6834201736806946, "ARES_Confidence_Interval": [0.616, 0.751], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.782, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.377}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.6_en_en.tsv", "ARES_Prediction": 0.7086666666666667, "ARES_Confidence_Interval": [0.641, 0.776], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.786, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.402}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.625_en_en.tsv", "ARES_Prediction": 0.6914362057448229, "ARES_Confidence_Interval": [0.624, 0.759], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.752, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.385}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.65_en_en.tsv", "ARES_Prediction": 0.7366666666666666, "ARES_Confidence_Interval": [0.669, 0.805], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.772, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.43}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.675_en_en.tsv", "ARES_Prediction": 0.7595724782899131, "ARES_Confidence_Interval": [0.692, 0.828], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.778, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.453}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(ar)_test_ratio_0.7_en_en.tsv", "ARES_Prediction": 0.7546666666666666, "ARES_Confidence_Interval": [0.687, 0.823], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.74, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.448}]]