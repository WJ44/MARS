[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.5780000000000001, "ARES_Confidence_Interval": [0.523, 0.633], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.896, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.548}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.5750901803607215, "ARES_Confidence_Interval": [0.52, 0.63], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.545}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5971342685370742, "ARES_Confidence_Interval": [0.542, 0.652], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.896, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.567}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5951302605210421, "ARES_Confidence_Interval": [0.54, 0.65], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.902, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.565}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.618, "ARES_Confidence_Interval": [0.563, 0.673], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.588}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.6392184368737476, "ARES_Confidence_Interval": [0.585, 0.694], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.609}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.656, "ARES_Confidence_Interval": [0.602, 0.71], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.888, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.626}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6893186372745491, "ARES_Confidence_Interval": [0.636, 0.743], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.908, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.659}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.686, "ARES_Confidence_Interval": [0.632, 0.74], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.656}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.5633333333333334, "ARES_Confidence_Interval": [0.509, 0.618], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.928, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.52}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.6084635938543754, "ARES_Confidence_Interval": [0.554, 0.663], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.565}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5984435537742151, "ARES_Confidence_Interval": [0.544, 0.653], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.928, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.555}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.6505477621910488, "ARES_Confidence_Interval": [0.597, 0.704], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.607}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.6313333333333333, "ARES_Confidence_Interval": [0.577, 0.685], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.588}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.660567802271209, "ARES_Confidence_Interval": [0.607, 0.714], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.617}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.6853333333333333, "ARES_Confidence_Interval": [0.632, 0.738], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.912, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.642}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.7367201068804275, "ARES_Confidence_Interval": [0.685, 0.788], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.693}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.7553333333333333, "ARES_Confidence_Interval": [0.704, 0.807], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.92, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.712}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.592, "ARES_Confidence_Interval": [0.533, 0.651], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.75, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.742}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.6215430861723447, "ARES_Confidence_Interval": [0.564, 0.679], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.741, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.772}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.6215430861723447, "ARES_Confidence_Interval": [0.564, 0.679], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.76, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.772}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.6035070140280561, "ARES_Confidence_Interval": [0.545, 0.662], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.754}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.608, "ARES_Confidence_Interval": [0.55, 0.666], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.81, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.758}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.6375751503006012, "ARES_Confidence_Interval": [0.58, 0.695], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.788}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.6639999999999999, "ARES_Confidence_Interval": [0.608, 0.72], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.804, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.814}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6576152304609219, "ARES_Confidence_Interval": [0.601, 0.714], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.832, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.808}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.692, "ARES_Confidence_Interval": [0.637, 0.747], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.822, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.842}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_de.tsv", "ARES_Prediction": 0.5613333333333334, "ARES_Confidence_Interval": [0.497, 0.626], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.824, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.368}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_de.tsv", "ARES_Prediction": 0.5821108884435537, "ARES_Confidence_Interval": [0.518, 0.647], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.808, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.389}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_de.tsv", "ARES_Prediction": 0.5660788243152972, "ARES_Confidence_Interval": [0.502, 0.63], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.79, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.373}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_de.tsv", "ARES_Prediction": 0.5941349365397461, "ARES_Confidence_Interval": [0.529, 0.659], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.778, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.401}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_de.tsv", "ARES_Prediction": 0.6133333333333333, "ARES_Confidence_Interval": [0.548, 0.678], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.792, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.42}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_de.tsv", "ARES_Prediction": 0.5921309285237141, "ARES_Confidence_Interval": [0.527, 0.657], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.758, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.399}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_de.tsv", "ARES_Prediction": 0.6373333333333333, "ARES_Confidence_Interval": [0.572, 0.702], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.774, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.444}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_de.tsv", "ARES_Prediction": 0.6462391449565799, "ARES_Confidence_Interval": [0.581, 0.711], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.762, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.453}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_de.tsv", "ARES_Prediction": 0.6533333333333333, "ARES_Confidence_Interval": [0.588, 0.719], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.744, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.46}]]