[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.5073333333333334, "ARES_Confidence_Interval": [0.454, 0.561], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.906, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.534}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5404676018704075, "ARES_Confidence_Interval": [0.487, 0.594], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.89, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.567}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5524916499665999, "ARES_Confidence_Interval": [0.5, 0.605], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.579}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.5845557782231129, "ARES_Confidence_Interval": [0.532, 0.637], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.884, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.611}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.5653333333333334, "ARES_Confidence_Interval": [0.513, 0.618], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.912, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.592}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.5625116900467603, "ARES_Confidence_Interval": [0.51, 0.615], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.9, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.589}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6113333333333334, "ARES_Confidence_Interval": [0.559, 0.663], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.638}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6466800267201069, "ARES_Confidence_Interval": [0.595, 0.698], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.673}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6513333333333334, "ARES_Confidence_Interval": [0.6, 0.702], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.678}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.5326666666666667, "ARES_Confidence_Interval": [0.48, 0.586], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.94, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.536}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5517768871075485, "ARES_Confidence_Interval": [0.499, 0.605], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.926, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.555}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.5878490313961257, "ARES_Confidence_Interval": [0.535, 0.64], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.932, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.591}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.5818370073480295, "ARES_Confidence_Interval": [0.529, 0.634], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.922, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.585}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.6106666666666667, "ARES_Confidence_Interval": [0.559, 0.663], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.614}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.631937207748831, "ARES_Confidence_Interval": [0.58, 0.684], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.902, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.635}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6586666666666667, "ARES_Confidence_Interval": [0.608, 0.71], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.924, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.662}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6840414161656647, "ARES_Confidence_Interval": [0.634, 0.735], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.952, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.687}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6906666666666667, "ARES_Confidence_Interval": [0.64, 0.741], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.934, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.694}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.6533333333333333, "ARES_Confidence_Interval": [0.598, 0.709], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.716, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.76}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.6468403473613894, "ARES_Confidence_Interval": [0.591, 0.702], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.727, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.754}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.6588643954575818, "ARES_Confidence_Interval": [0.604, 0.714], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.754, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.766}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.6548563794255177, "ARES_Confidence_Interval": [0.6, 0.71], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.758, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.762}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.6573333333333333, "ARES_Confidence_Interval": [0.602, 0.712], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.768, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.764}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.6508483633934535, "ARES_Confidence_Interval": [0.596, 0.706], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.8, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.758}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6793333333333333, "ARES_Confidence_Interval": [0.625, 0.734], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.8, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.786}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6809084836339345, "ARES_Confidence_Interval": [0.627, 0.735], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.824, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.788}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.6733333333333333, "ARES_Confidence_Interval": [0.619, 0.728], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.84, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.78}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.5_de_en.tsv", "ARES_Prediction": 0.6086666666666667, "ARES_Confidence_Interval": [0.545, 0.673], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.838, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.462}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.525_de_en.tsv", "ARES_Prediction": 0.5815364061456245, "ARES_Confidence_Interval": [0.518, 0.646], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.525, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.806, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.435}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.55_de_en.tsv", "ARES_Prediction": 0.6236205744822979, "ARES_Confidence_Interval": [0.559, 0.688], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.551, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.818, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.477}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.575_de_en.tsv", "ARES_Prediction": 0.6536806947227789, "ARES_Confidence_Interval": [0.589, 0.718], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.575, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.816, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.507}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.6_de_en.tsv", "ARES_Prediction": 0.6746666666666667, "ARES_Confidence_Interval": [0.611, 0.739], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.6, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.872, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.528}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.625_de_en.tsv", "ARES_Prediction": 0.685744822979292, "ARES_Confidence_Interval": [0.622, 0.75], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.81, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.539}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.65_de_en.tsv", "ARES_Prediction": 0.6666666666666667, "ARES_Confidence_Interval": [0.602, 0.731], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.794, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.52}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.675_de_en.tsv", "ARES_Prediction": 0.6877488309953239, "ARES_Confidence_Interval": [0.624, 0.752], "Number_of_Examples_in_Evaluation_Set": 499, "Ground_Truth_Performance": 0.675, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.798, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.541}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_(de)_test_ratio_0.7_de_en.tsv", "ARES_Prediction": 0.7246666666666666, "ARES_Confidence_Interval": [0.661, 0.788], "Number_of_Examples_in_Evaluation_Set": 500, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.786, "Annotated_Examples_used_for_PPI": 300, "Pre_PPI_Score": 0.578}]]