[[{"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.5108424908424908, "ARES_Confidence_Interval": [0.449, 0.573], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.871, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5407129094412331, "ARES_Confidence_Interval": [0.478, 0.603], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.91, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5306060606060606, "ARES_Confidence_Interval": [0.467, 0.594], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.5664135021097046, "ARES_Confidence_Interval": [0.502, 0.63], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.896, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.5769306930693069, "ARES_Confidence_Interval": [0.512, 0.642], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.914, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.6033333333333333, "ARES_Confidence_Interval": [0.538, 0.668], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.904, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.6259523809523809, "ARES_Confidence_Interval": [0.561, 0.691], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.918, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6541387856257744, "ARES_Confidence_Interval": [0.59, 0.719], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Context_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.6558974358974359, "ARES_Confidence_Interval": [0.591, 0.721], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.935, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.45157509157509157, "ARES_Confidence_Interval": [0.39, 0.513], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.4895761078998073, "ARES_Confidence_Interval": [0.427, 0.552], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.916, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5615151515151515, "ARES_Confidence_Interval": [0.499, 0.624], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.921, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.5635864978902954, "ARES_Confidence_Interval": [0.5, 0.627], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.937, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.5736963696369637, "ARES_Confidence_Interval": [0.509, 0.638], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.5893814432989691, "ARES_Confidence_Interval": [0.524, 0.654], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.938, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.604047619047619, "ARES_Confidence_Interval": [0.539, 0.669], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.939, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6385006195786865, "ARES_Confidence_Interval": [0.574, 0.703], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.929, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Relevance_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.6587179487179488, "ARES_Confidence_Interval": [0.594, 0.724], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.931, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.4866666666666667, "ARES_Confidence_Interval": [0.417, 0.557], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.712, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5141233140655106, "ARES_Confidence_Interval": [0.445, 0.584], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.702, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5124242424242424, "ARES_Confidence_Interval": [0.442, 0.583], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.752, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.5056540084388186, "ARES_Confidence_Interval": [0.434, 0.577], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.769, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.5485478547854786, "ARES_Confidence_Interval": [0.479, 0.618], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.762, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.5304810996563574, "ARES_Confidence_Interval": [0.459, 0.602], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.784, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.5688095238095239, "ARES_Confidence_Interval": [0.499, 0.638], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.782, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.5693804213135069, "ARES_Confidence_Interval": [0.499, 0.639], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.829, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Answer_Faithfulness_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.5674358974358975, "ARES_Confidence_Interval": [0.497, 0.638], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.831, "Annotated_Examples_used_for_PPI": 300}], [{"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.5_ar_en.tsv", "ARES_Prediction": 0.554945054945055, "ARES_Confidence_Interval": [0.496, 0.614], "Number_of_Examples_in_Evaluation_Set": 364, "Ground_Truth_Performance": 0.5, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.901, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.525_ar_en.tsv", "ARES_Prediction": 0.5635838150289018, "ARES_Confidence_Interval": [0.504, 0.623], "Number_of_Examples_in_Evaluation_Set": 346, "Ground_Truth_Performance": 0.526, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.928, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.55_ar_en.tsv", "ARES_Prediction": 0.5727272727272728, "ARES_Confidence_Interval": [0.512, 0.634], "Number_of_Examples_in_Evaluation_Set": 330, "Ground_Truth_Performance": 0.552, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.918, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.575_ar_en.tsv", "ARES_Prediction": 0.6012658227848101, "ARES_Confidence_Interval": [0.54, 0.663], "Number_of_Examples_in_Evaluation_Set": 316, "Ground_Truth_Performance": 0.576, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.943, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.6_ar_en.tsv", "ARES_Prediction": 0.6105610561056105, "ARES_Confidence_Interval": [0.548, 0.673], "Number_of_Examples_in_Evaluation_Set": 303, "Ground_Truth_Performance": 0.601, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.944, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.625_ar_en.tsv", "ARES_Prediction": 0.6666666666666666, "ARES_Confidence_Interval": [0.605, 0.728], "Number_of_Examples_in_Evaluation_Set": 291, "Ground_Truth_Performance": 0.625, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.89, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.65_ar_en.tsv", "ARES_Prediction": 0.6392857142857142, "ARES_Confidence_Interval": [0.576, 0.703], "Number_of_Examples_in_Evaluation_Set": 280, "Ground_Truth_Performance": 0.65, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.961, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.675_ar_en.tsv", "ARES_Prediction": 0.6877323420074349, "ARES_Confidence_Interval": [0.625, 0.75], "Number_of_Examples_in_Evaluation_Set": 269, "Ground_Truth_Performance": 0.677, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.952, "Annotated_Examples_used_for_PPI": 300}, {"Label_Column": "Language_Consistency_Label", "Evaluation_Set": "multilingual_data/mlqa_test_ratio_0.7_ar_en.tsv", "ARES_Prediction": 0.6961538461538461, "ARES_Confidence_Interval": [0.633, 0.759], "Number_of_Examples_in_Evaluation_Set": 260, "Ground_Truth_Performance": 0.7, "ARES_LLM_Judge_Accuracy_on_Ground_Truth_Labels": 0.965, "Annotated_Examples_used_for_PPI": 300}]]